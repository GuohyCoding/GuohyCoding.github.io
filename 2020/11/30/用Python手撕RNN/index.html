<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="用Python手撕RNN"><meta name="keywords" content="Python,Deep Learning,Recurrent Neural Network"><meta name="author" content="Hongyi Guo"><meta name="copyright" content="Hongyi Guo"><title>用Python手撕RNN | Hongyi's</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?77a8c2d0f4e8f862b652458768e3fc6a";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn"><span class="toc-number">1.</span> <span class="toc-text"> RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的循环"><span class="toc-number">1.1.</span> <span class="toc-text"> 神经网络的循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基于时间的反向传播"><span class="toc-number">1.2.</span> <span class="toc-text"> 基于时间的反向传播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn的实现"><span class="toc-number">2.</span> <span class="toc-text"> RNN的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#单层rnn的实现"><span class="toc-number">2.1.</span> <span class="toc-text"> 单层RNN的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#time-rnn的实现"><span class="toc-number">2.2.</span> <span class="toc-text"> Time RNN的实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#将rnn用于处理时序数据"><span class="toc-number">3.</span> <span class="toc-text"> 将RNN用于处理时序数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#rnnlm"><span class="toc-number">3.1.</span> <span class="toc-text"> RNNLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnnlm的实现"><span class="toc-number">3.2.</span> <span class="toc-text"> RNNLM的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnnlm的学习"><span class="toc-number">3.3.</span> <span class="toc-text"> RNNLM的学习</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/avatar.jpg"></div><div class="author-info__name text-center">Hongyi Guo</div><div class="author-info__description text-center">Nothing is impossible to a willing heart.</div><div class="follow-button"><a href="https://github.com/GuohyCoding">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">19</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">19</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/59f7d76d99a79.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Hongyi's</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">用Python手撕RNN</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-30</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Technology/">Technology</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Technology/A-Beginner-s-Guide-To-Neural-Network/">A Beginner's Guide To Neural Network</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.6k</span><span class="post-meta__separator">|</span><span>Reading time: 14 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>前面介绍的几个神经网络都是前馈型神经网络，前馈的意思是网络的传播方向是单向的。但是在NLP任务中，输入可能是一句话，词在句子中具有位置信息，即句子是时序数据，前馈神经网络无法充分学习时序数据的性质，而<strong>RNN</strong>（Recurrent Neural Network，<strong>循环神经网络</strong>）则可以较好的处理时序数据。</p>
<a id="more"></a>
<h1 id="rnn"><a class="markdownIt-Anchor" href="#rnn"></a> RNN</h1>
<h2 id="神经网络的循环"><a class="markdownIt-Anchor" href="#神经网络的循环"></a> 神经网络的循环</h2>
<p>循环的意思就是数据的流通形成一个环路，使得数据在神经网络中不断的循环，从而一边记住过去的数据，一遍更新最新的数据，RNN的特征就是拥有这样的一个环路，下图展现了RNN的这个环路</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/101.png" alt></p>
<p>在上图中，左边是数据在整体RNN层循环的大体示意，右边是RNN层的展开，不同的时刻对应的不同的RNN层，时刻t的输入数据是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，时序数据<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>…</mo><mtext> </mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_0, x_1, \dots, x_t, \dots)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mclose">)</span></span></span></span>会被输入到RNN层中，输出为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>h</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>…</mo><mtext> </mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h_0, h_1, \dots, h_t, \dots)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mclose">)</span></span></span></span>。可以看到，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的数据中包含有最开始的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的数据，各个时刻的 RNN 层接收传给该层的输入和前一个 RNN 层的输出，然后据此计算当前时刻的输出，时刻t的输出计算公式为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>W</mi><mi>h</mi></msub><mo>+</mo><msub><mi>x</mi><mi>t</mi></msub><msub><mi>W</mi><mi>x</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t=tanh(h_{t-1}W_h+x_tW_x+b)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span></span></p>
<p>在上式中，RNN有两个权重，分别是将输入x转化为输出h的权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">W_x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和将前一个RNN层的输出转化为当前时刻的输出权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">W_h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，此外还有偏置b。在该式中，首先执行矩阵的乘积计算，然后使用tanh函数变换他们的和，因此<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的结果是由<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>计算得来的，可以说记忆了前面的计算结果，即RNN具有储存记忆的功能。</p>
<h2 id="基于时间的反向传播"><a class="markdownIt-Anchor" href="#基于时间的反向传播"></a> 基于时间的反向传播</h2>
<p>上一节中介绍了RNN的正向传播，与前馈神经网络不同的是，RNN可视为在水平方向上延伸的神经网络，数据有两个方向的传递，一个是向垂直方向上的下一层传递，一个是向水平方向上不同时刻的RNN层进行传递。与RNN正向传播的过程类似，RNN的反向传播也是按时间顺序展开的反向传播，称为<strong>Backpropagation Through Time</strong>（基于时间的反向传播），简称 <strong>BPTT</strong>，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/103.png" alt></p>
<p>在计算BPTT时有个需要解决的问题，那就是随着时序的时间跨度增大时，BPTT的计算量会呈指数级的增大，同时BPTT的梯度也会不稳定，因此需要对学习长时序的数据进行截断，即<strong>Truncated BPTT</strong>（截断的 BPTT），具体来说，就是将时间轴方向上过长的网络在合适的位置进行截断，从而创建多个小型网络，然后对截出来的小型网络执行误差反向传播法，需要注意的是截断的只是反向传播，正向传播的连接依然被维持，Truncated BPTT数据的处理顺序如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/107.png" alt></p>
<h1 id="rnn的实现"><a class="markdownIt-Anchor" href="#rnn的实现"></a> RNN的实现</h1>
<h2 id="单层rnn的实现"><a class="markdownIt-Anchor" href="#单层rnn的实现"></a> 单层RNN的实现</h2>
<p>回顾一下RNN正向传播的数学表达式</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>W</mi><mi>h</mi></msub><mo>+</mo><msub><mi>x</mi><mi>t</mi></msub><msub><mi>W</mi><mi>x</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t=tanh(h_{t-1}W_h+x_tW_x+b)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span></span></p>
<p>上式中用到的运算有tanh、乘法和加法，都是之前介绍的几个神经网络中用到的操作，所以正向传播和反向传播的代码也和之前的神经网络差不多，代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, Wx, Wh, b)</span>:</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, h_prev)</span>:</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b</span><br><span class="line">        h_next = np.tanh(t)</span><br><span class="line"></span><br><span class="line">        self.cache = (x, h_prev, h_next)</span><br><span class="line">        <span class="keyword">return</span> h_next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dh_next)</span>:</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        x, h_prev, h_next = self.cache</span><br><span class="line"></span><br><span class="line">        dt = dh_next * (<span class="number">1</span> - h_next ** <span class="number">2</span>)</span><br><span class="line">        db = np.sum(dt, axis=<span class="number">0</span>)</span><br><span class="line">        dWh = np.dot(h_prev.T, dt)</span><br><span class="line">        dh_prev = np.dot(dt, Wh.T)</span><br><span class="line">        dWx = np.dot(x.T, dt)</span><br><span class="line">        dx = np.dot(dt, Wx.T)</span><br><span class="line"></span><br><span class="line">        self.grads[<span class="number">0</span>][...] = dWx</span><br><span class="line">        self.grads[<span class="number">1</span>][...] = dWh</span><br><span class="line">        self.grads[<span class="number">2</span>][...] = db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx, dh_prev</span><br></pre></td></tr></table></figure>
<p><code>RNN</code> 的初始化方法接收两个权重参数和一个偏置参数。这里，将通过函数参数传进来的模型参数设置为列表类型的成员变量 <code>params</code>。然后，以各个参数对应的形状初始化梯度，并保存在 <code>grads</code> 中。最后，使用 <code>None</code> 对反向传播时要用到的中间数据 <code>cache</code> 进行初始化，正向传播的 <code>forward(x, h_prev)</code> 方法接收两个参数：从下方输入的 <code>x</code> 和从左边输入的 <code>h_prev</code>，反向传播的计算图如下所示，结合代码就很容易理解了。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/113.png" alt></p>
<h2 id="time-rnn的实现"><a class="markdownIt-Anchor" href="#time-rnn的实现"></a> Time RNN的实现</h2>
<p>前面已经介绍过了，RNN可视为在水平方向上延伸的神经网络，在水平方向上由连续多个单层RNN组合起来的网络，我们称之为<strong>Time RNN</strong>，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/114.png" alt></p>
<p>Truncated BPTT的处理过程就是将每一个截断的隐藏状态h保存在成员变量中，在一个截断的传播中会用到这个成员变量，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/115.png" alt></p>
<p>TimeRNN的实现代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeRNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, Wx, Wh, b, stateful=False)</span>:</span></span><br><span class="line">        self.params = [Wx, Wh, b]</span><br><span class="line">        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.h, self.dh = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        self.stateful = stateful</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, h)</span>:</span></span><br><span class="line">        self.h = h</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_state</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.h = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xs)</span>:</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, D = xs.shape</span><br><span class="line">        D, H = Wx.shape</span><br><span class="line"></span><br><span class="line">        self.layers = []</span><br><span class="line">        hs = np.empty((N, T, H), dtype=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.stateful <span class="keyword">or</span> self.h <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.h = np.zeros((N, H), dtype=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">            layer = RNN(*self.params)</span><br><span class="line">            self.h = layer.forward(xs[:, t, :], self.h)</span><br><span class="line">            hs[:, t, :] = self.h</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hs</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dhs)</span>:</span></span><br><span class="line">        Wx, Wh, b = self.params</span><br><span class="line">        N, T, H = dhs.shape</span><br><span class="line">        D, H = Wx.shape    </span><br><span class="line"></span><br><span class="line">        dxs = np.empty((N, T, D), dtype=<span class="string">'f'</span>)</span><br><span class="line">        dh = <span class="number">0</span></span><br><span class="line">        grads = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            dx, dh = layer.backward(dhs[:, t, :] + dh) <span class="comment"># 求和后的梯度</span></span><br><span class="line">            dxs[:, t, :] = dx</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(layer.grads):</span><br><span class="line">                grads[i] += grad</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grads):</span><br><span class="line">            self.grads[i][...] = grad</span><br><span class="line">        self.dh = dh</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dxs</span><br></pre></td></tr></table></figure>
<p>下面来分别介绍这段实现代码，初始化方法的参数有权重、偏置和布尔型（<code>True/False</code>）的 <code>stateful</code>。一个成员变量 <code>layers</code> 在列表中保存多个 RNN 层，另一个成员变量，<code>h</code> 保存调用 <code>forward()</code> 方法时的最后一个 RNN 层的隐藏状态。参数中的 <code>stateful</code> 是“有状态”的意思，当 <code>stateful</code> 为 <code>True</code> 时，无论时序数据多长，Time RNN 层的正向传播都可以不中断地进行，而当 <code>stateful</code> 为 <code>False</code> 时，每次调用 Time RNN 层的 <code>forward()</code> 时，第一个 RNN 层的隐藏状态都会被初始化为零矩阵（所有元素均为 0 的矩阵）。</p>
<p>正向传播的 <code>forward(xs)</code> 方法从下方获取输入 <code>xs</code>，<code>xs</code> 囊括了T个时序数据。因此，如果批大小是 <code>N</code>，输入向量的维数是 <code>D</code>，则 <code>xs</code> 的形状为 <code>(N,T,D)</code>。</p>
<p>TimeRNN的反向传播<code>backward(dhs)</code>结合下图来进行理解，在反向传播中，将从上游（输出侧的层）传来的梯度记为<code>dhs</code>，将流向下游的梯度记为 <code>dxs</code>,因为这里我们进行的是 Truncated BPTT，所以不需要流向这个块上一时刻的反向传播，只需要将流向上一时刻的隐藏状态的梯度存放在成员变量 <code>dh</code> 中。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/116.png" alt></p>
<h1 id="将rnn用于处理时序数据"><a class="markdownIt-Anchor" href="#将rnn用于处理时序数据"></a> 将RNN用于处理时序数据</h1>
<h2 id="rnnlm"><a class="markdownIt-Anchor" href="#rnnlm"></a> RNNLM</h2>
<p>之前介绍的word2vec其实是一种语言模型，通过语言模型能够给出词语序列发生的概率，也就是说使用概率来评估一个词语序列发生的可能性，即在多大程度上是自然的词语序列。介绍完了单层RNN和TimeRNN的实现过程，接下来还需要像搭积木一样来搭建完整的用于处理时序数据的RNN语言模型，我们这里称之为<strong>RNNLM(RNN Language Model）</strong>。</p>
<p>RNNLM同样用到了之前介绍过的Affine层、Embedding 层、Softmax层，RNNLM的全貌图如下所示，右图展示了在时间轴上展开的形式。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/118.png" alt></p>
<p>经过第一层的 Embedding 层可以将语料库中的词语ID转化为词语的分布式表示（词向量），然后将这个词向量输入到RNN层中，RNN向下一层输出隐藏状态，同时也向下一时刻的单层RNN输出隐藏状态，最终RNN向下一层输出的隐藏状态经过Affine层，传给Softmax层。</p>
<h2 id="rnnlm的实现"><a class="markdownIt-Anchor" href="#rnnlm的实现"></a> RNNLM的实现</h2>
<p>RNNLM中的Affine层、Embedding 层、Softmax层和之前介绍的神经网络的各层不太一样，因为要处理时序数据，同样也要使用Time Embedding 层、Time Affine 层、Time Softmax层等来实现整体处理时序数据的层，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/120.png" alt></p>
<p>Time Embedding 层、Time Affine 层、Time Softmax层的处理比Time RNN层要简单许多，因为不需要在水平方向上传递数据，只需要别处理各个时刻的数据即可，下面是Time Embedding 层、Time Affine 层、Time Softmax层的实现代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeEmbedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W)</span>:</span></span><br><span class="line">        self.params = [W]</span><br><span class="line">        self.grads = [np.zeros_like(W)]</span><br><span class="line">        self.layers = <span class="literal">None</span></span><br><span class="line">        self.W = W</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xs)</span>:</span></span><br><span class="line">        N, T = xs.shape</span><br><span class="line">        V, D = self.W.shape</span><br><span class="line"></span><br><span class="line">        out = np.empty((N, T, D), dtype=<span class="string">'f'</span>)</span><br><span class="line">        self.layers = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">            layer = Embedding(self.W)</span><br><span class="line">            out[:, t, :] = layer.forward(xs[:, t])</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">        N, T, D = dout.shape</span><br><span class="line"></span><br><span class="line">        grad = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">            layer = self.layers[t]</span><br><span class="line">            layer.backward(dout[:, t, :])</span><br><span class="line">            grad += layer.grads[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        self.grads[<span class="number">0</span>][...] = grad</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeAffine</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W, b)</span>:</span></span><br><span class="line">        self.params = [W, b]</span><br><span class="line">        self.grads = [np.zeros_like(W), np.zeros_like(b)]</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        N, T, D = x.shape</span><br><span class="line">        W, b = self.params</span><br><span class="line"></span><br><span class="line">        rx = x.reshape(N*T, <span class="number">-1</span>)</span><br><span class="line">        out = np.dot(rx, W) + b</span><br><span class="line">        self.x = x</span><br><span class="line">        <span class="keyword">return</span> out.reshape(N, T, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">        x = self.x</span><br><span class="line">        N, T, D = x.shape</span><br><span class="line">        W, b = self.params</span><br><span class="line"></span><br><span class="line">        dout = dout.reshape(N*T, <span class="number">-1</span>)</span><br><span class="line">        rx = x.reshape(N*T, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        db = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">        dW = np.dot(rx.T, dout)</span><br><span class="line">        dx = np.dot(dout, W.T)</span><br><span class="line">        dx = dx.reshape(*x.shape)</span><br><span class="line"></span><br><span class="line">        self.grads[<span class="number">0</span>][...] = dW</span><br><span class="line">        self.grads[<span class="number">1</span>][...] = db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeSoftmaxWithLoss</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line">        self.ignore_label = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xs, ts)</span>:</span></span><br><span class="line">        N, T, V = xs.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ts.ndim == <span class="number">3</span>:  <span class="comment"># 在监督标签为one-hot向量的情况下</span></span><br><span class="line">            ts = ts.argmax(axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        mask = (ts != self.ignore_label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 按批次大小和时序大小进行整理（reshape）</span></span><br><span class="line">        xs = xs.reshape(N * T, V)</span><br><span class="line">        ts = ts.reshape(N * T)</span><br><span class="line">        mask = mask.reshape(N * T)</span><br><span class="line"></span><br><span class="line">        ys = softmax(xs)</span><br><span class="line">        ls = np.log(ys[np.arange(N * T), ts])</span><br><span class="line">        ls *= mask  <span class="comment"># 与ignore_label相应的数据将损失设为0</span></span><br><span class="line">        loss = -np.sum(ls)</span><br><span class="line">        loss /= mask.sum()</span><br><span class="line"></span><br><span class="line">        self.cache = (ts, ys, mask, (N, T, V))</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout=<span class="number">1</span>)</span>:</span></span><br><span class="line">        ts, ys, mask, (N, T, V) = self.cache</span><br><span class="line"></span><br><span class="line">        dx = ys</span><br><span class="line">        dx[np.arange(N * T), ts] -= <span class="number">1</span></span><br><span class="line">        dx *= dout</span><br><span class="line">        dx /= mask.sum()</span><br><span class="line">        dx *= mask[:, np.newaxis]  <span class="comment"># 与ignore_label相应的数据将梯度设为0</span></span><br><span class="line"></span><br><span class="line">        dx = dx.reshape((N, T, V))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>这里需要介绍一下<code>TimeSoftmaxWithLoss</code>这个类，在这个类中一并实现了Time Softmax 层和 Time  Cross Entropy Error 层，称之为Time Softmax with Loss 层，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/122.png" alt></p>
<p>计算完每个时序数据的损失后，将它们加在一起取个平均就行了，得到的值最为最终的损失，计算的式子如下</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mo stretchy="false">(</mo><msub><mi>L</mi><mn>0</mn></msub><mo>+</mo><msub><mi>L</mi><mn>0</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>L</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=\frac{1}{T}(L_0+L_0+\dots+L_{T-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>介绍完了Time Embedding 层、Time Affine 层、Time Time Softmax with Loss 层之后，只需将它们拼在一起就行了，将RNNLM的实现定义为<code>SimpleRnnlm</code>类，实现代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleRnnlm</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, wordvec_size, hidden_size)</span>:</span></span><br><span class="line">        V, D, H = vocab_size, wordvec_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        embed_W = (rn(V, D) / <span class="number">100</span>).astype(<span class="string">'f'</span>)</span><br><span class="line">        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype(<span class="string">'f'</span>)</span><br><span class="line">        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype(<span class="string">'f'</span>)</span><br><span class="line">        rnn_b = np.zeros(H).astype(<span class="string">'f'</span>)</span><br><span class="line">        affine_W = (rn(H, V) / np.sqrt(H)).astype(<span class="string">'f'</span>)</span><br><span class="line">        affine_b = np.zeros(V).astype(<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成层</span></span><br><span class="line">        self.layers = [</span><br><span class="line">            TimeEmbedding(embed_W),</span><br><span class="line">            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=<span class="literal">True</span>),</span><br><span class="line">            TimeAffine(affine_W, affine_b)</span><br><span class="line">        ]</span><br><span class="line">        self.loss_layer = TimeSoftmaxWithLoss()</span><br><span class="line">        self.rnn_layer = self.layers[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将所有的权重和梯度整理到列表中</span></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xs, ts)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            xs = layer.forward(xs)</span><br><span class="line">        loss = self.loss_layer.forward(xs, ts)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout=<span class="number">1</span>)</span>:</span></span><br><span class="line">        dout = self.loss_layer.backward(dout)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> reversed(self.layers):</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> dout</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">reset_state</span><span class="params">(self)</span>:</span></span><br><span class="line">    	self.rnn_layer.reset_state()</span><br></pre></td></tr></table></figure>
<p>这段代码值得注意的地方是权重和偏置初始化的部分，在上面的初始化代码中，RNN 层和 Affine 层使用了<strong>Xavier 初始值</strong>，Xavier 初始值的含义是在上一层的节点数是n的情况下，使用标准差为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><msqrt><mi>n</mi></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\sqrt{n}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.383108em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6258665em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8059050000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-2.765905em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.234095em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>的分布作为初始值。</p>
<h2 id="rnnlm的学习"><a class="markdownIt-Anchor" href="#rnnlm的学习"></a> RNNLM的学习</h2>
<p>学习的代码使用了 PTB 数据集的前1000个单词（训练数据）进行学习，PTB数据集很容易的可以获取到，学习的代码如下所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定超参数</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">wordvec_size = <span class="number">100</span></span><br><span class="line">hidden_size = <span class="number">100</span> <span class="comment"># RNN的隐藏状态向量的元素个数</span></span><br><span class="line">time_size = <span class="number">5</span> <span class="comment"># Truncated BPTT的时间跨度大小</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">max_epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入训练数据（缩小了数据集）</span></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">'train'</span>)</span><br><span class="line">corpus_size = <span class="number">1000</span></span><br><span class="line">corpus = corpus[:corpus_size]</span><br><span class="line">vocab_size = int(max(corpus) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">xs = corpus[:<span class="number">-1</span>] <span class="comment"># 输入</span></span><br><span class="line">ts = corpus[<span class="number">1</span>:] <span class="comment"># 输出（监督标签）</span></span><br><span class="line">data_size = len(xs)</span><br><span class="line">print(<span class="string">'corpus size: %d, vocabulary size: %d'</span> % (corpus_size, vocab_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习用的参数</span></span><br><span class="line">max_iters = data_size // (batch_size * time_size)</span><br><span class="line">time_idx = <span class="number">0</span></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line">loss_count = <span class="number">0</span></span><br><span class="line">ppl_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模型</span></span><br><span class="line">model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)</span><br><span class="line">optimizer = SGD(lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ❶ 计算读入mini-batch的各笔样本数据的开始位置</span></span><br><span class="line">jump = (corpus_size - <span class="number">1</span>) // batch_size</span><br><span class="line">offsets = [i * jump <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(max_epoch):</span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        <span class="comment"># ❷ 获取mini-batch</span></span><br><span class="line">        batch_x = np.empty((batch_size, time_size), dtype=<span class="string">'i'</span>)</span><br><span class="line">        batch_t = np.empty((batch_size, time_size), dtype=<span class="string">'i'</span>)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(time_size):</span><br><span class="line">            <span class="keyword">for</span> i, offset <span class="keyword">in</span> enumerate(offsets):</span><br><span class="line">                batch_x[i, t] = xs[(offset + time_idx) % data_size]</span><br><span class="line">                batch_t[i, t] = ts[(offset + time_idx) % data_size]</span><br><span class="line">            time_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算梯度，更新参数</span></span><br><span class="line">        loss = model.forward(batch_x, batch_t)</span><br><span class="line">        model.backward()</span><br><span class="line">        optimizer.update(model.params, model.grads)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        loss_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ❸ 各个epoch的困惑度评价</span></span><br><span class="line">    ppl = np.exp(total_loss / loss_count)</span><br><span class="line">    print(<span class="string">'| epoch %d | perplexity %.2f'</span> % (epoch+<span class="number">1</span>, ppl))</span><br><span class="line">    ppl_list.append(float(ppl))</span><br><span class="line">    total_loss, loss_count = <span class="number">0</span>, <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这段代码有三处需要注意</p>
<ul>
<li>利用mini-batch分批训练数据</li>
<li>使用了SGD进行优化</li>
<li>采用困惑度作为评价标准</li>
</ul>
<p>前两点之前的文章中介绍有，这里介绍一下为什么RNNLM这个语言模型要选取困惑度作为评价标准。困惑度简单来说就是预测将要出现词语的概率的倒数，假如**you say goodbye and i say hello.**这句话，语言模型预测you的下一个单词是say的概率为0.8，则困惑度就是这个概率的倒数，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>0.8</mn></mfrac><mo>=</mo><mn>1.25</mn></mrow><annotation encoding="application/x-tex">\frac{1}{0.8}=1.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">.</span><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span></span></span></span>，困惑度越接近1则表明语言模型预测的越准确。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Hongyi Guo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://guohongyi.com/2020/11/30/用Python手撕RNN/">https://guohongyi.com/2020/11/30/用Python手撕RNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Recurrent-Neural-Network/">Recurrent Neural Network</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/12/07/用Python手撕LSTM/"><i class="fa fa-chevron-left">  </i><span>用Python手撕LSTM</span></a></div><div class="next-post pull-right"><a href="/2020/11/19/用Python手撕word2vec/"><span>用Python手撕word2vec</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'Rh8B8qrnJ3cukY5PM5TJziVq-gzGzoHsz',
  appKey:'zHaQG2PQjKnL9mXX2EAbKFDB',
  placeholder:'Please leave your footprints',
  avatar:'retro',
  guest_info:guest_info,
  pageSize:'5',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/59f7d76d99a79.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By Hongyi Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://guohongyi.com">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>