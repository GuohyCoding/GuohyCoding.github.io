<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="用Python手撕word2vec"><meta name="keywords" content="Python,Deep Learning,word2vec,NLP"><meta name="author" content="Hongyi Guo"><meta name="copyright" content="Hongyi Guo"><title>用Python手撕word2vec | Hongyi's</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?77a8c2d0f4e8f862b652458768e3fc6a";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#词的one-hot表示"><span class="toc-number">1.</span> <span class="toc-text"> 词的one-hot表示</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#word2vec的模型"><span class="toc-number">2.</span> <span class="toc-text"> word2vec的模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#cbow"><span class="toc-number">2.1.</span> <span class="toc-text"> CBOW</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skip-gram"><span class="toc-number">2.2.</span> <span class="toc-text"> skip-gram</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#word2vec的高速化"><span class="toc-number">3.</span> <span class="toc-text"> word2vec的高速化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">3.1.</span> <span class="toc-text"> Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#负采样"><span class="toc-number">3.2.</span> <span class="toc-text"> 负采样</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#word2vec的实现"><span class="toc-number">4.</span> <span class="toc-text"> word2vec的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding层的实现"><span class="toc-number">4.1.</span> <span class="toc-text"> Embedding层的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#负采样的实现"><span class="toc-number">4.2.</span> <span class="toc-text"> 负采样的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cbow模型的实现"><span class="toc-number">4.3.</span> <span class="toc-text"> CBOW模型的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skip-gram-模型的实现"><span class="toc-number">4.4.</span> <span class="toc-text"> skip-gram 模型的实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#word2vec的学习"><span class="toc-number">5.</span> <span class="toc-text"> word2vec的学习</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/avatar.jpg"></div><div class="author-info__name text-center">Hongyi Guo</div><div class="author-info__description text-center">Nothing is impossible to a willing heart.</div><div class="follow-button"><a href="https://github.com/GuohyCoding">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">17</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">17</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/59f7d76d99a79.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Hongyi's</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">用Python手撕word2vec</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Technology/">Technology</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Technology/A-Beginner-s-Guide-To-Neural-Network/">A Beginner's Guide To Neural Network</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.4k</span><span class="post-meta__separator">|</span><span>Reading time: 16 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>在NLP任务中，最细粒度的单位是词，词组成句子，句子组成段落，段落组成篇章，因此在做NLP任务的时候，需要用某种形式来把词表示出来，word2vec用向量来表示词语，能够来表征词与词的映射关系，举一个例子来说，word2vec能够发现man - woman = king - queen的关系， word2vec的本质其是一个双层的浅神经网络，那么word2vec到底是如何实现的呢？</p>
<a id="more"></a>
<h1 id="词的one-hot表示"><a class="markdownIt-Anchor" href="#词的one-hot表示"></a> 词的one-hot表示</h1>
<p>神经网络是无法直接词的，例如英语中的you或say这样的单词，要用神经网络来处理词，需要先将词转化为机器能够识别的向量形式，一种最简单的转换方式为 one-hot 表示，在one-hot表示中，只有一个元素是1，其他的元素都是0。</p>
<p>用&quot;<strong>You say goodbye and I say hello.</strong>&quot;这句话来介绍one-hot表示，在这句话中一共有7个单词（包含末尾的句号），因此需要准备元素个数与词汇个数相等的向量，并将单词 ID 对应的元素设为 1，其他元素设为 0，其中you和goodbye的表示如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/050.png" alt></p>
<p>神经网络的输入层的神经元个数也因此能够固定下来，输入层由 7 个神经元表示，分别对应于 7 个单词，如下图所示，神经元为1的地方用黑色绘制，为0的地方用白色绘制。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/051.png" alt></p>
<h1 id="word2vec的模型"><a class="markdownIt-Anchor" href="#word2vec的模型"></a> word2vec的模型</h1>
<p>可能到这里，有小伙伴就会很奇怪，讲了这么多word2vec到底是干嘛用的呢？其实，word2vec是一个进行推理任务的浅层神经网络（两层），基于推理的方式不同，word2vec有两种神经网络的模型，一种是<strong>CBOW</strong>（<strong>continuous bag-of-words</strong>）模型，另一种是<strong>skip-gram</strong>模型。</p>
<h2 id="cbow"><a class="markdownIt-Anchor" href="#cbow"></a> CBOW</h2>
<p>CBOW模型的推理是通过上下文来预测目标词，即给出一句话，从这句话中挖去一个词，然后给出周围的词，来预测挖去的这个词应该是什么，如下图预测&quot;?&quot;处单词的推理过程。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/048.png" alt></p>
<p>CBOW模型接收上下文信息作为输入，并输出（可能出现的）各个单词的出现概率，例如上下文用 <code>['you', 'goodbye']</code> 这样的单词列表表示， CBOW 模型有两个输入层（因为我们对上下文仅考虑两个单词，所以输入层有两个，如果对上下文考虑N个单词，则输入层也会有N个），经过中间层到达输出层。这里，从输入层到中间层的变换由相同的全连接层（权重为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>i</mi></msub><mi>n</mi></mrow><annotation encoding="application/x-tex">W_in</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">n</span></span></span></span>）完成，从中间层到输出层神经元的变换由另一个全连接层（权重为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub><mi>u</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">W_out</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span></span></span></span> ）完成。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/058.png" alt></p>
<p>假设语料库中只有&quot;<strong>You say goodbye and I say hello.</strong>&quot;七个单词，则上图中这个输出层有 7 个神经元，神经元的个数为语料库中所有词的个数，输出层的神经元是各个单词的得分，它的值越大，说明对应单词的出现概率就越高。得分是指在被解释为概率之前的值，对这些得分应用 Softmax 函数，就可以得到概率，下图所示了CBOW模型的正反向的传播，MatMul层表示输入层或中间层与权重的乘积，Softmax with Loss 层表示之前介绍过的Softmax 层和 Cross Entropy Error 层的合并。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/066.png" alt></p>
<h2 id="skip-gram"><a class="markdownIt-Anchor" href="#skip-gram"></a> skip-gram</h2>
<p>skip-gram 模型是反转了 CBOW 模型处理的上下文和目标词的模型，即通过目标词来预测上下文，用下图进行对比就一清二楚了。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/069.png" alt></p>
<p>CBOW 模型从上下文的多个单词预测中间的单词（目标词），而 skip-gram 模型则从中间的单词（目标词）预测周围的多个单词（上下文），那么skip-gram 模型的网络结构就可以用下图来表示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/070.png" alt></p>
<p>这里需要注意的是，skip-gram 模型的损失函数与CBOW模型不同，skip-gram 模型的输入层只有一个，输出层的数量则与上下文的单词个数相等，因此，首先要分别求出各个输出层的损失（通过 Softmax with Loss 层等），然后将它们加起来作为最后的损失，skip-gram 模型的损失函数可以表示为下式</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>l</mi><mi>o</mi><mi>g</mi><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=-\frac{1}{T}\sum_{t=1}^T(log\space p(w_{t-1}|w_t) + log\space p(w_{t+1}|w_t))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<h1 id="word2vec的高速化"><a class="markdownIt-Anchor" href="#word2vec的高速化"></a> word2vec的高速化</h1>
<h2 id="embedding"><a class="markdownIt-Anchor" href="#embedding"></a> Embedding</h2>
<p>在one-hot 表示中，输入层的神经元是语料库中词语的个数，假设语料库中含有100万个词语，中间层的神经元个数是100，则输入层与权重的乘积就如下图所示，可以看到计算这个巨大向量和权重矩阵的乘积要花费大量的计算资源，并且仔细观察下图，可以看到输入层与权重的乘积所做的无非就是将矩阵的某个特定的行提取出来，直觉上并不需要进行如此大量的矩阵乘法运算。因此，需要使用一种新的方式抽取单词对应行的向量，Embedding就应运而生。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/073.png" alt></p>
<p>在NLP领域，单词的密集向量表示称为<strong>词嵌入</strong>（word embedding）或者单词的<strong>分布式表示</strong>（distributed representation），Embedding其实做的工作很简单，就是提取特定行的内容，即Embedding 的正向传播只是从权重矩阵 <code>W</code> 中提取特定的行，并将该特定行的神经元原样传给下一层。那么Embedding的反向传播就是从上一层（输出侧的层）传过来的梯度将原样传给下一层（输入侧的层），如下图所示，图中 <code>idx</code> 表示需要提取的行的索引。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/074.png" alt></p>
<p>注意，在反向传播中，有可能<code>idx</code>的元素是重复的，比如当<code>idx</code>为<code>[0, 2, 0, 4]</code> 时会发生下图所示的问题，因此应该把 <code>dh</code> 各行的值累加到 <code>dW</code> 的对应行中。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/075.png" alt></p>
<h2 id="负采样"><a class="markdownIt-Anchor" href="#负采样"></a> 负采样</h2>
<p>不论是CBOW模型还是skip-gram 模型，推断结果都是预测正确目标词的概率，也就是说如果语料库中有100万个词，那么输出层的神经元个数也是100万个，然后从这100万个神经元中选取概率最大的一个词（CBOW模型），这同样也会出现one-hot表示的那样会耗费大量的计算资源。</p>
<p>其实，除了中间层与输出权重庞大的矩阵乘积之外，还有一点会影响效率的就是<code>Softmax</code>层的计算，随着语料库词语个数的增加，<code>Softmax</code>的计算量也会增加，还是以100万个词的语料库为例，<code>Softmax</code>层的计算将变成下面的式子</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>l</mi></msub><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>1000000</mn></munderover><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">y_l=\frac{exp(s_k)}{\sum_{i=1}^{1000000}exp(s_i)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.5707180000000003em;vertical-align:-1.143718em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.155992em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.143718em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>第<code>k</code>个元素（词）的 <code>Softmax</code>的计算式（各个元素的得分为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)，因为假定词汇量是 100 万个，所分母需要进行 100 万次的 exp 计算，这个计算也与词汇量成正比，因此，需要一个可以替代 <code>Softmax</code>的“轻量”的计算。</p>
<p>那有没有效率更好的计算方法呢？其中的一种解决方式就是<strong>负采样（negative sampling）</strong>，负采样简单来说就是把多分类问题转化为二分类问题，以CBOW模型举例，推理过程是从100万个词中预测正确目标词的概率，这明显是个庞大的100万个类别的多分类问题，那能不能变成二分类问题呢？比如句子&quot;<strong>You say goodbye and I say hello.</strong>&quot;用you和goodbye来预测say时，将推理过程变成”<strong>目标词是say吗</strong>“这个问题，就把多分类问题变成了二分类问题，即是say和不是say，那么输出层就只可以只用一个或两个神经元来代替，用一个神经元结果就是目标词是say的概率，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/077.png" alt></p>
<p>到此时呢，仅仅是推理了正例（目标词）的概率，还不确定负例（预测错误的词）会产生怎样的结果，在之前的例子中，上下文是 you 和 goodbye，目标词是 say，我们到目前为止只是对正例 say 进行了二分类，但是对 say 之外的负例一无所知，而我们真正要做的事情是，对于正例（say），使 Sigmoid 层的输出接近 1；对于负例（say 以外的单词），使 Sigmoid 层的输出接近 0。</p>
<p>那么有必要对所有负例都进行推理学习吗？答案显示是没必要，如果全部负例都推理学习，那就又回到了100万个类别的多分类问题了，作为一种近似方法，我们仅需要选取特定个数的负例进行推理学习，只采用少数负例这就是负采样的含义，如下图所示，负例的个数可以很少（例如10个负例），注意该图中加了Embedding Dot层，Embedding Dot层与Embedding 层有所不同，主要是Embedding Dot层需要将中间层的结果与输出权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub><mi>u</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">W_out</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span></span></span></span>进行内积。</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/086.png" alt></p>
<p>总而言之，负采样方法既可以求将正例作为目标词时的损失，同时也可以采样（选出）若干个负例，对这些负例求损失。然后，将这些数据（正例和采样出来的负例）的损失加起来，将其结果作为最终的损失，负采样输出侧的传播如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/087.png" alt></p>
<h1 id="word2vec的实现"><a class="markdownIt-Anchor" href="#word2vec的实现"></a> word2vec的实现</h1>
<p>上面介绍了word2vec的一些基本的概念，其实就是一个简单的浅层（两层）神经网络，只是对于输入层和输出层需要做些高速化的处理，下面介绍word2vec的代码实现。</p>
<h2 id="embedding层的实现"><a class="markdownIt-Anchor" href="#embedding层的实现"></a> Embedding层的实现</h2>
<p>输入侧的Embedding 层的实现较为简单，如下面代码所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W)</span>:</span></span><br><span class="line">        self.params = [W]</span><br><span class="line">        self.grads = [np.zeros_like(W)]</span><br><span class="line">        self.idx = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        W, = self.params</span><br><span class="line">        self.idx = idx</span><br><span class="line">        out = W[idx]</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">        dW, = self.grads</span><br><span class="line">        dW[...] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, word_id <span class="keyword">in</span> enumerate(self.idx):</span><br><span class="line">    		dW[word_id] += dout[i]</span><br><span class="line">    	<span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>输出侧的Embedding 层需要与输出权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub><mi>u</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">W_out</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span></span></span></span>进行矩阵乘积，如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/082.png" alt></p>
<p>因此，输出侧的Embedding 层要和dot运算层合并起来进行处理，结合后的Embedding Dot 层的代码如下所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingDot</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W)</span>:</span></span><br><span class="line">        self.embed = Embedding(W)</span><br><span class="line">        self.params = self.embed.params</span><br><span class="line">        self.grads = self.embed.grads</span><br><span class="line">        self.cache = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, h, idx)</span>:</span></span><br><span class="line">        target_W = self.embed.forward(idx)</span><br><span class="line">        out = np.sum(target_W * h, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.cache = (h, target_W)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">        h, target_W = self.cache</span><br><span class="line">        dout = dout.reshape(dout.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        dtarget_W = dout * h</span><br><span class="line">        self.embed.backward(dtarget_W)</span><br><span class="line">        dh = dout * target_W</span><br><span class="line">        <span class="keyword">return</span> dh</span><br></pre></td></tr></table></figure>
<h2 id="负采样的实现"><a class="markdownIt-Anchor" href="#负采样的实现"></a> 负采样的实现</h2>
<p>负采样首先要做的事随机抽取负例，随机抽取负例有个技巧，就是要让语料库中经常出现的单词容易被抽到，让语料库中不经常出现的单词难以被抽到。所以，通常的抽取方式是基于语料库中单词使用频率的采样方法会先计算语料库中各个单词的出现次数，并将其表示为“概率分布”，然后使用这个概率分布对单词进行采样， 随机抽取负例由下面的<code>UnigramSampler</code> 类来实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnigramSampler</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, corpus, power, sample_size)</span>:</span></span><br><span class="line">        self.sample_size = sample_size</span><br><span class="line">        self.vocab_size = <span class="literal">None</span></span><br><span class="line">        self.word_p = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        counts = collections.Counter()</span><br><span class="line">        <span class="keyword">for</span> word_id <span class="keyword">in</span> corpus:</span><br><span class="line">            counts[word_id] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        vocab_size = len(counts)</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line"></span><br><span class="line">        self.word_p = np.zeros(vocab_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(vocab_size):</span><br><span class="line">            self.word_p[i] = counts[i]</span><br><span class="line"></span><br><span class="line">        self.word_p = np.power(self.word_p, power)</span><br><span class="line">        self.word_p /= np.sum(self.word_p)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_negative_sample</span><span class="params">(self, target)</span>:</span></span><br><span class="line">        batch_size = target.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> GPU:</span><br><span class="line">            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                p = self.word_p.copy()</span><br><span class="line">                target_idx = target[i]</span><br><span class="line">                p[target_idx] = <span class="number">0</span></span><br><span class="line">                p /= p.sum()</span><br><span class="line">                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=<span class="literal">False</span>, p=p)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 在用GPU(cupy）计算时，优先速度</span></span><br><span class="line">            <span class="comment"># 有时目标词存在于负例中</span></span><br><span class="line">            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),</span><br><span class="line">                                               replace=<span class="literal">True</span>, p=self.word_p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> negative_sample</span><br></pre></td></tr></table></figure>
<p>在<code>UnigramSampler</code> 类中，简单的介绍一下<code>np.random.choice()</code>方法，<code>np.random.choice()</code> 可以用于随机抽样，如果指定 <code>size</code> 参数，将执行多次采样，如果指定 <code>replace=False</code>，将进行无放回采样，通过给参数 <code>p</code> 指定表示概率分布的列表，因此通过语料库中词不同的概率，将进行基于概率分布的负采样，负采样的实现就如下面代码所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NegativeSamplingLoss</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W, corpus, power=<span class="number">0.75</span>, sample_size=<span class="number">5</span>)</span>:</span></span><br><span class="line">        self.sample_size = sample_size</span><br><span class="line">        self.sampler = UnigramSampler(corpus, power, sample_size)</span><br><span class="line">        self.loss_layers = [SigmoidWithLoss() <span class="keyword">for</span> _ <span class="keyword">in</span> range(sample_size + <span class="number">1</span>)]</span><br><span class="line">        self.embed_dot_layers = [EmbeddingDot(W) <span class="keyword">for</span> _ <span class="keyword">in</span> range(sample_size + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.embed_dot_layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, h, target)</span>:</span></span><br><span class="line">        batch_size = target.shape[<span class="number">0</span>]</span><br><span class="line">        negative_sample = self.sampler.get_negative_sample(target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正例的正向传播</span></span><br><span class="line">        score = self.embed_dot_layers[<span class="number">0</span>].forward(h, target)</span><br><span class="line">        correct_label = np.ones(batch_size, dtype=np.int32)</span><br><span class="line">        loss = self.loss_layers[<span class="number">0</span>].forward(score, correct_label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 负例的正向传播</span></span><br><span class="line">        negative_label = np.zeros(batch_size, dtype=np.int32)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.sample_size):</span><br><span class="line">            negative_target = negative_sample[:, i]</span><br><span class="line">            score = self.embed_dot_layers[<span class="number">1</span> + i].forward(h, negative_target)</span><br><span class="line">            loss += self.loss_layers[<span class="number">1</span> + i].forward(score, negative_label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout=<span class="number">1</span>)</span>:</span></span><br><span class="line">        dh = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> l0, l1 <span class="keyword">in</span> zip(self.loss_layers, self.embed_dot_layers):</span><br><span class="line">            dscore = l0.backward(dout)</span><br><span class="line">            dh += l1.backward(dscore)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dh</span><br></pre></td></tr></table></figure>
<p>同样要注意的是，负例的损失要进行累加而不是覆盖。</p>
<h2 id="cbow模型的实现"><a class="markdownIt-Anchor" href="#cbow模型的实现"></a> CBOW模型的实现</h2>
<p>有了Embedding 层和 Negative Sampling Loss 层，CBOW模型的实现就很简单了，CBOW模型的代码如下所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, window_size, corpus)</span>:</span></span><br><span class="line">        V, H = vocab_size, hidden_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        W_in = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">'f'</span>)</span><br><span class="line">        W_out = <span class="number">0.01</span> * np.random.randn(V, H).astype(<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成层</span></span><br><span class="line">        self.in_layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span> * window_size):</span><br><span class="line">            layer = Embedding(W_in)  <span class="comment"># 使用Embedding层</span></span><br><span class="line">            self.in_layers.append(layer)</span><br><span class="line">        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=<span class="number">0.75</span>, sample_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将所有的权重和梯度整理到列表中</span></span><br><span class="line">        layers = self.in_layers + [self.ns_loss]</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将单词的分布式表示设置为成员变量</span></span><br><span class="line">        self.word_vecs = W_in</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, contexts, target)</span>:</span></span><br><span class="line">        h = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.in_layers):</span><br><span class="line">            h += layer.forward(contexts[:, i])</span><br><span class="line">        h *= <span class="number">1</span> / len(self.in_layers)</span><br><span class="line">        loss = self.ns_loss.forward(h, target)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout=<span class="number">1</span>)</span>:</span></span><br><span class="line">        dout = self.ns_loss.backward(dout)</span><br><span class="line">        dout *= <span class="number">1</span> / len(self.in_layers)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.in_layers:</span><br><span class="line">            layer.backward(dout)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>这个初始化方法有 4 个参数。<code>vocab_size</code> 是词汇量，<code>hidden_size</code> 是中间层的神经元个数，<code>corpus</code> 是词 ID 列表。另外，通过 <code>window_size</code> 指定上下文的大小，即上下文包含多少个周围单词。如果 <code>window_size</code> 是 2，则目标词的左右 2 个词（共 4 个词）将成为上下文。</p>
<h2 id="skip-gram-模型的实现"><a class="markdownIt-Anchor" href="#skip-gram-模型的实现"></a> skip-gram 模型的实现</h2>
<p>同CBOW模型类似，skip-gram 模型的代码如下所示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGram</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, window_size, corpus)</span>:</span></span><br><span class="line">        V, H = vocab_size, hidden_size</span><br><span class="line">        rn = np.random.randn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        W_in = <span class="number">0.01</span> * rn(V, H).astype(<span class="string">'f'</span>)</span><br><span class="line">        W_out = <span class="number">0.01</span> * rn(V, H).astype(<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成层</span></span><br><span class="line">        self.in_layer = Embedding(W_in)</span><br><span class="line">        self.loss_layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span> * window_size):</span><br><span class="line">            layer = NegativeSamplingLoss(W_out, corpus, power=<span class="number">0.75</span>, sample_size=<span class="number">5</span>)</span><br><span class="line">            self.loss_layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将所有的权重和梯度整理到列表中</span></span><br><span class="line">        layers = [self.in_layer] + self.loss_layers</span><br><span class="line">        self.params, self.grads = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            self.params += layer.params</span><br><span class="line">            self.grads += layer.grads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将单词的分布式表示设置为成员变量</span></span><br><span class="line">        self.word_vecs = W_in</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, contexts, target)</span>:</span></span><br><span class="line">        h = self.in_layer.forward(target)</span><br><span class="line"></span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.loss_layers):</span><br><span class="line">            loss += layer.forward(h, contexts[:, i])</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout=<span class="number">1</span>)</span>:</span></span><br><span class="line">        dh = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.loss_layers):</span><br><span class="line">            dh += layer.backward(dout)</span><br><span class="line">        self.in_layer.backward(dh)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h1 id="word2vec的学习"><a class="markdownIt-Anchor" href="#word2vec的学习"></a> word2vec的学习</h1>
<p>有了上面word2vec神经网络代码的实现，现在就要进行神经网络的学习了，学习的过程与ANN和CNN类似，用到了之前的<code>Trainer类</code>和<code>Adam</code>优化方法，word2vec学习的过程以CBOW模型为例，skip-gram 模型与此类似，CBOW模型学习的代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定超参数</span></span><br><span class="line">window_size = <span class="number">5</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">max_epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">'train'</span>)</span><br><span class="line">vocab_size = len(word_to_id)</span><br><span class="line"></span><br><span class="line">contexts, target = create_contexts_target(corpus, window_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模型等</span></span><br><span class="line">model = CBOW(vocab_size, hidden_size, window_size, corpus)</span><br><span class="line">optimizer = Adam()  </span><br><span class="line">trainer = Trainer(model, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始学习</span></span><br><span class="line">trainer.fit(contexts, target, max_epoch, batch_size)</span><br><span class="line">trainer.plot()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存必要数据，以便后续使用</span></span><br><span class="line">word_vecs = model.word_vecs</span><br><span class="line"></span><br><span class="line">params = &#123;&#125;</span><br><span class="line">params[<span class="string">'word_vecs'</span>] = word_vecs.astype(np.float16)</span><br><span class="line">params[<span class="string">'word_to_id'</span>] = word_to_id</span><br><span class="line">params[<span class="string">'id_to_word'</span>] = id_to_word</span><br><span class="line">pkl_file = <span class="string">'cbow_params.pkl'</span></span><br><span class="line"><span class="keyword">with</span> open(pkl_file, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(params, f, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p><code>create_contexts_target</code>函数的作用是生成上下文和目标词的函数（输入参数和标签），该函数的实现代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_contexts_target</span><span class="params">(corpus, window_size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    target = corpus[window_size:-window_size] <span class="comment"># 去掉首尾window_size个元素</span></span><br><span class="line">    contexts = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(window_size, len(corpus)-window_size):</span><br><span class="line">        cs = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(-window_size, window_size + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            cs.append(corpus[idx + t])</span><br><span class="line">        contexts.append(cs)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> np.array(contexts), np.array(target)</span><br></pre></td></tr></table></figure>
<p>此外，在学习结束后，取出权重（输入侧的权重），并保存在文件中以备后用（用于单词和单词 ID 之间的转化的字典也一起保存）。这里，使用 Python 的 pickle 功能进行文件保存。pickle 可以将 Python 代码中的对象保存到文件中（或者从文件中读取对象）。</p>
<p>至此，word2vec的实现就到一段落了，将词转换为向量是非常重要的一项工作，这极大的提高了下游NLP任务的可操作性，因为如果可以将自然语言转化为向量，就可以使用常规的机器学习方法了（神经网络、SVM 等），就如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2020/DeepLearningScratch/091.png" alt></p>
<p>但是呢，word2vec是2013年Google公司发布的，现在（2020年）的NLP任务已经换了处理方向，大多是基于预训练模型（GPT、BERT等）和微调（Fine-tuning），然后在对不同的下游NLP任务采取不同的学习训练方式，在以后有机会会介绍一下GPT或者BERT的预训练模型，虽然现在采用这些预训练模型的NLP任务的准确度越来越高，但是像word2vec或者是tf-idf这些过时的处理方式也是有其用武之地的。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Hongyi Guo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://guohongyi.com/2020/11/22/用Python手撕word2vec/">https://guohongyi.com/2020/11/22/用Python手撕word2vec/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/word2vec/">word2vec</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="next-post pull-right"><a href="/2020/11/12/用Python手撕CNN/"><span>用Python手撕CNN</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'Rh8B8qrnJ3cukY5PM5TJziVq-gzGzoHsz',
  appKey:'zHaQG2PQjKnL9mXX2EAbKFDB',
  placeholder:'Please leave your footprints',
  avatar:'retro',
  guest_info:guest_info,
  pageSize:'5',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/59f7d76d99a79.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By Hongyi Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://guohongyi.com">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>