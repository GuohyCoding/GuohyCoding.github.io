<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="用Python手撕CNN"><meta name="keywords" content="Python,Deep Learning,Convolutional Neural Network"><meta name="author" content="Hongyi Guo"><meta name="copyright" content="Hongyi Guo"><title>用Python手撕CNN | Hongyi's</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?77a8c2d0f4e8f862b652458768e3fc6a";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn的整体结构"><span class="toc-number">1.</span> <span class="toc-text"> CNN的整体结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积层"><span class="toc-number">1.1.</span> <span class="toc-text"> 卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积运算"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 卷积运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#填充"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#步幅"><span class="toc-number">1.1.3.</span> <span class="toc-text"> 步幅</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多维数据的卷积运算"><span class="toc-number">1.1.4.</span> <span class="toc-text"> 多维数据的卷积运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#池化层"><span class="toc-number">1.2.</span> <span class="toc-text"> 池化层</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#卷积层和池化层的实现"><span class="toc-number">2.</span> <span class="toc-text"> 卷积层和池化层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积层的实现"><span class="toc-number">2.1.</span> <span class="toc-text"> 卷积层的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#池化层的实现"><span class="toc-number">2.2.</span> <span class="toc-text"> 池化层的实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn的实现"><span class="toc-number">3.</span> <span class="toc-text"> CNN的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#简单cnn的实现"><span class="toc-number">3.1.</span> <span class="toc-text"> 简单CNN的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#复杂cnn的实现"><span class="toc-number">3.2.</span> <span class="toc-text"> 复杂CNN的实现</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/avatar.jpg"></div><div class="author-info__name text-center">Hongyi Guo</div><div class="author-info__description text-center">Nothing is impossible to a willing heart.</div><div class="follow-button"><a href="https://github.com/GuohyCoding">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">18</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">18</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/59f7d76d99a79.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Hongyi's</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">用Python手撕CNN</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Technology/">Technology</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Technology/A-Beginner-s-Guide-To-Neural-Network/">A Beginner's Guide To Neural Network</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.4k</span><span class="post-meta__separator">|</span><span>Reading time: 18 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>上一节介绍了ANN，ANN可以说是最简单的神经网络了，但是ANN却包含了神经网络需要的基础框架，之后的网络都是在此基础上删删补补。本节介绍CNN（Convolutional Neural Network，卷积神经网络），CNN常被用于图像识别、语音识别等各种场合。</p>
<a id="more"></a>
<h1 id="cnn的整体结构"><a class="markdownIt-Anchor" href="#cnn的整体结构"></a> CNN的整体结构</h1>
<p>CNN和ANN一样，可以像堆乐高积木一样一层一层的来组装，在CNN中，新出现了卷积层（Convolution）和池化层（Pooling）。</p>
<h2 id="卷积层"><a class="markdownIt-Anchor" href="#卷积层"></a> 卷积层</h2>
<p>在ANN中出现的Affine层也被称为全连接层，即相邻层的所有神经元之间都有连接，但是全连接层有个问题，就是不能表征数据的形状，举个例子，在图像识别中，图像通常是由长、高和通道来表示的3维数据，在全连接层中，需要将这3维数据拉平成1维数据，这样就会丢失了图像关于形状的相关信息。</p>
<p>而卷积层则保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层，因此，在 CNN 中，可能会正确理解到图像具有的形状信息。</p>
<h3 id="卷积运算"><a class="markdownIt-Anchor" href="#卷积运算"></a> 卷积运算</h3>
<p>卷积层进行的处理就是卷积运算，卷积运算相当于图像处理中的<strong>滤波器运算</strong>或<strong>核运算</strong>，以下面的一个例子来介绍一下卷积运算</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/118.png" alt></p>
<p>在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是 (4, 4)，滤波器大小是 (3, 3)，输出大小是 (2, 2)，下图具体介绍了卷积运算的计算顺序。</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/119.png" alt></p>
<p>对于输入数据，卷积运算以一定间隔滑动滤波器的窗口，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和，如果存在偏置的话，则偏置会被加到滤波器的所有元素上。</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/120.png" alt></p>
<h3 id="填充"><a class="markdownIt-Anchor" href="#填充"></a> 填充</h3>
<p>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如 0 等），这称为<strong>填充</strong>（padding），是卷积运算中经常会用到的处理。在下图中，对大小为 (4, 4) 的输入数据应用了幅度为 1 的填充。“幅度为 1 的填充”是指用幅度为 1 像素的 0 填充周围。</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/121.png" alt></p>
<p>通过填充，大小为 (4, 4) 的输入数据变成了 (6, 6) 的形状。然后，应用大小为 (3, 3) 的滤波器，生成了大小为 (4, 4) 的输出数据。</p>
<h3 id="步幅"><a class="markdownIt-Anchor" href="#步幅"></a> 步幅</h3>
<p>应用滤波器的位置间隔称为<strong>步幅</strong>（stride），之前的例子中步幅都是 1，如果将步幅设为 2，，应用滤波器的窗口的间隔变为 2 个元素，就像下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/122.png" alt></p>
<p>综合填充和步幅可以发现，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，假设输入大小为 (<em>H</em>, <em>W</em>)，滤波器大小为 (<em>FH</em>, <em>FW</em>)，输出大小为 (<em>OH</em>, <em>OW</em>)，填充为 <em>P</em>，步幅为 S，则输出大小的计算公式如下所示</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mi>H</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mo>+</mo><mn>2</mn><mi>P</mi><mo>−</mo><mi>F</mi><mi>H</mi></mrow><mi>S</mi></mfrac><mo>+</mo><mn>1</mn><mspace linebreak="newline"></mspace><mi>O</mi><mi>W</mi><mo>=</mo><mfrac><mrow><mi>W</mi><mo>+</mo><mn>2</mn><mi>P</mi><mo>−</mo><mi>F</mi><mi>W</mi></mrow><mi>S</mi></mfrac><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">OH=\frac{H+2P-FH}{S}+1 \\
OW=\frac{W+2P-FW}{S}+1
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.04633em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.04633em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></span></p>
<h3 id="多维数据的卷积运算"><a class="markdownIt-Anchor" href="#多维数据的卷积运算"></a> 多维数据的卷积运算</h3>
<p>多维数据的卷积运算与2维数据的卷积运算同理，只是滤波器的维度和输入数据的维度相同，如下图所示，通道数为 <em>C</em>、高度为 <em>H</em>、长度为 <em>W</em> 的数据的形状可以写成（<em>C</em>, <em>H</em>, <em>W</em>），滤波器的维度相同，为（<em>C</em>, <em>FH</em>, <em>FW</em>），但是，为了要在通道方向上也拥有多个卷积运算的输出，就需要用到多个滤波器（权重），假设滤波器的个数为FN，则输出的特征图也生成了 <em>FN</em> 个，果将这 <em>FN</em> 个特征图汇集在一起，就得到了形状为 (<em>FN</em>, <em>OH</em>, <em>OW</em>) 的方块，并且每个通道只有一个偏置，偏置的形状是 (<em>FN</em>, 1, 1)，滤波器的输出结果的形状是 (<em>FN</em>, <em>OH</em>, <em>OW</em>)，这两个方块相加时，要对滤波器的输出结果 (<em>FN</em>, <em>OH</em>, <em>OW</em>) 按通道加上相同的偏置值，将这个方块传给下一层，就是 CNN 的处理流。</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/127.png" alt></p>
<h2 id="池化层"><a class="markdownIt-Anchor" href="#池化层"></a> 池化层</h2>
<p>池化层的作用是缩小高、长方向上的空间运算，如下图所示进行将 2 × 2 的区域集约成 1 个元素的处理，缩小空间大小。</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/129.png" alt></p>
<p>池化层可以分为最大池化和平均池化等，上图中是最大池化，即选取目标区域中最大的元素作为输出，池化层具有以下特征：</p>
<ul>
<li><strong>没有要学习的参数</strong>：池化层和卷积层不同，没有要学习的参数，池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数</li>
<li><strong>通道数不发生变化</strong>：经过池化运算，输入数据和输出数据的通道数不会发生变化</li>
<li><strong>对微小的位置变化具有鲁棒性（健壮）</strong>：输入数据发生微小偏差时，池化仍会返回相同的结</li>
</ul>
<h1 id="卷积层和池化层的实现"><a class="markdownIt-Anchor" href="#卷积层和池化层的实现"></a> 卷积层和池化层的实现</h1>
<p>前面简单的介绍了卷积层和池化层的原理，现在来用Python来实现这两个层，同ANN，这两个层也需要forward<code>和</code>backward方法。</p>
<h2 id="卷积层的实现"><a class="markdownIt-Anchor" href="#卷积层的实现"></a> 卷积层的实现</h2>
<p>在利用Python实现卷积层时，用到了im2col函数，im2col函数将输入数据展开以适合滤波器（权重），不需要我们用重复好几层for循环语句来实现，im2col函数的定义为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># input_data：由（数据量，通道，高，长）的 4 维数组构成的输入数据</span></span><br><span class="line"><span class="comment"># filter_h：滤波器的高</span></span><br><span class="line"><span class="comment"># filter_w：滤波器的长</span></span><br><span class="line"><span class="comment"># stride：步幅</span></span><br><span class="line"><span class="comment"># pad：填充</span></span><br><span class="line">im2col (input_data, filter_h, filter_w, stride=<span class="number">1</span>, pad=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>使用im2col来实现卷积层的代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Convolution</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W, b, stride=<span class="number">1</span>, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">    self.W = W</span><br><span class="line">    self.b = b</span><br><span class="line">    self.stride = stride</span><br><span class="line">    self.pad = pad</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    FN, C, FH, FW = self.W.shape</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    out_h = int(<span class="number">1</span> + (H + <span class="number">2</span>*self.pad - FH) / self.stride)</span><br><span class="line">    out_w = int(<span class="number">1</span> + (W + <span class="number">2</span>*self.pad - FW) / self.stride)</span><br><span class="line"></span><br><span class="line">    col = im2col(x, FH, FW, self.stride, self.pad)</span><br><span class="line">    col_W = self.W.reshape(FN, <span class="number">-1</span>).T <span class="comment"># 滤波器的展开</span></span><br><span class="line">    out = np.dot(col, col_W) + self.b</span><br><span class="line"></span><br><span class="line">    out = out.reshape(N, out_h, out_w, <span class="number">-1</span>).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">    FN, C, FH, FW = self.W.shape</span><br><span class="line">    dout = dout.transpose(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).reshape(<span class="number">-1</span>, FN)</span><br><span class="line"></span><br><span class="line">    self.db = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    self.dW = np.dot(self.col.T, dout)</span><br><span class="line">    self.dW = self.dW.transpose(<span class="number">1</span>, <span class="number">0</span>).reshape(FN, C, FH, FW)</span><br><span class="line"></span><br><span class="line">    dcol = np.dot(dout, self.col_W.T)</span><br><span class="line">    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>注意，在forward的实现中，最后会将输出大小转换为合适的形状。转换时使用了 NumPy 的 transpose函数，transpose会更改多维数组的轴的顺序，在进行卷积层的反向传播时，必须进行 im2col的逆处理，卷积层的反向传播和上一节ANN的Affine 层的实现方式一样。</p>
<h2 id="池化层的实现"><a class="markdownIt-Anchor" href="#池化层的实现"></a> 池化层的实现</h2>
<p>池化层和卷积层有点不同，就是在池化的情况下，在通道方向上是独立的，池化需要按通道单独展开，就如下图所示</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/137.png" alt></p>
<p>池化层的代码实现也是大同小异</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pooling</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pool_h, pool_w, stride=<span class="number">1</span>, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.pool_h = pool_h</span><br><span class="line">        self.pool_w = pool_w</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.pad = pad</span><br><span class="line">        </span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.arg_max = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        N, C, H, W = x.shape</span><br><span class="line">        out_h = int(<span class="number">1</span> + (H - self.pool_h) / self.stride)</span><br><span class="line">        out_w = int(<span class="number">1</span> + (W - self.pool_w) / self.stride)</span><br><span class="line"></span><br><span class="line">        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)</span><br><span class="line">        col = col.reshape(<span class="number">-1</span>, self.pool_h*self.pool_w)</span><br><span class="line"></span><br><span class="line">        arg_max = np.argmax(col, axis=<span class="number">1</span>)</span><br><span class="line">        out = np.max(col, axis=<span class="number">1</span>)</span><br><span class="line">        out = out.reshape(N, out_h, out_w, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.x = x</span><br><span class="line">        self.arg_max = arg_max</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">        dout = dout.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        pool_size = self.pool_h * self.pool_w</span><br><span class="line">        dmax = np.zeros((dout.size, pool_size))</span><br><span class="line">        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()</span><br><span class="line">        dmax = dmax.reshape(dout.shape + (pool_size,)) </span><br><span class="line">        </span><br><span class="line">        dcol = dmax.reshape(dmax.shape[<span class="number">0</span>] * dmax.shape[<span class="number">1</span>] * dmax.shape[<span class="number">2</span>], <span class="number">-1</span>)</span><br><span class="line">        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h1 id="cnn的实现"><a class="markdownIt-Anchor" href="#cnn的实现"></a> CNN的实现</h1>
<p>已经实现了卷积层和池化层，现在来组合这些层，搭建进行手写数字识别的 CNN。</p>
<h2 id="简单cnn的实现"><a class="markdownIt-Anchor" href="#简单cnn的实现"></a> 简单CNN的实现</h2>
<p>首先构建一个3层的CNN网络结果，网络结果如下所示</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/138.png" alt></p>
<p>可以看出网络的构成是<strong>Convolution - ReLU - Pooling -Affine - ReLU - Affine - Softmax</strong>，整个步骤也和之前介绍的ANN一样，这里就直接贴上代码了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleConvNet</span>:</span></span><br><span class="line">    <span class="string">"""简单的ConvNet</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    conv - relu - pool - affine - relu - affine - softmax</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    input_size : 输入大小（MNIST的情况下为784）</span></span><br><span class="line"><span class="string">    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）</span></span><br><span class="line"><span class="string">    output_size : 输出大小（MNIST的情况下为10）</span></span><br><span class="line"><span class="string">    activation : 'relu' or 'sigmoid'</span></span><br><span class="line"><span class="string">    weight_init_std : 指定权重的标准差（e.g. 0.01）</span></span><br><span class="line"><span class="string">        指定'relu'或'he'的情况下设定“He的初始值”</span></span><br><span class="line"><span class="string">        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param=&#123;<span class="string">'filter_num'</span>:<span class="number">30</span>, <span class="string">'filter_size'</span>:<span class="number">5</span>, <span class="string">'pad'</span>:<span class="number">0</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size=<span class="number">100</span>, output_size=<span class="number">10</span>, weight_init_std=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        filter_num = conv_param[<span class="string">'filter_num'</span>]</span><br><span class="line">        filter_size = conv_param[<span class="string">'filter_size'</span>]</span><br><span class="line">        filter_pad = conv_param[<span class="string">'pad'</span>]</span><br><span class="line">        filter_stride = conv_param[<span class="string">'stride'</span>]</span><br><span class="line">        input_size = input_dim[<span class="number">1</span>]</span><br><span class="line">        conv_output_size = (input_size - filter_size + <span class="number">2</span>*filter_pad) / filter_stride + <span class="number">1</span></span><br><span class="line">        pool_output_size = int(filter_num * (conv_output_size/<span class="number">2</span>) * (conv_output_size/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(filter_num, input_dim[<span class="number">0</span>], filter_size, filter_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(filter_num)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(pool_output_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W3'</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b3'</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成层</span></span><br><span class="line">        self.layers = OrderedDict()</span><br><span class="line">        self.layers[<span class="string">'Conv1'</span>] = Convolution(self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>],</span><br><span class="line">                                           conv_param[<span class="string">'stride'</span>], conv_param[<span class="string">'pad'</span>])</span><br><span class="line">        self.layers[<span class="string">'Relu1'</span>] = Relu()</span><br><span class="line">        self.layers[<span class="string">'Pool1'</span>] = Pooling(pool_h=<span class="number">2</span>, pool_w=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layers[<span class="string">'Affine1'</span>] = Affine(self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>])</span><br><span class="line">        self.layers[<span class="string">'Relu2'</span>] = Relu()</span><br><span class="line">        self.layers[<span class="string">'Affine2'</span>] = Affine(self.params[<span class="string">'W3'</span>], self.params[<span class="string">'b3'</span>])</span><br><span class="line"></span><br><span class="line">        self.last_layer = SoftmaxWithLoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers.values():</span><br><span class="line">            x = layer.forward(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        <span class="string">"""求损失函数</span></span><br><span class="line"><span class="string">        参数x是输入数据、t是标签数据</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> self.last_layer.forward(y, t)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, x, t, batch_size=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> t.ndim != <span class="number">1</span> : t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        acc = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(int(x.shape[<span class="number">0</span>] / batch_size)):</span><br><span class="line">            tx = x[i*batch_size:(i+<span class="number">1</span>)*batch_size]</span><br><span class="line">            tt = t[i*batch_size:(i+<span class="number">1</span>)*batch_size]</span><br><span class="line">            y = self.predict(tx)</span><br><span class="line">            y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">            acc += np.sum(y == tt) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> acc / x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        <span class="string">"""求梯度（数值微分）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x : 输入数据</span></span><br><span class="line"><span class="string">        t : 标签数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        具有各层的梯度的字典变量</span></span><br><span class="line"><span class="string">            grads['W1']、grads['W2']、...是各层的权重</span></span><br><span class="line"><span class="string">            grads['b1']、grads['b2']、...是各层的偏置</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss_w = <span class="keyword">lambda</span> w: self.loss(x, t)</span><br><span class="line"></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>):</span><br><span class="line">            grads[<span class="string">'W'</span> + str(idx)] = numerical_gradient(loss_w, self.params[<span class="string">'W'</span> + str(idx)])</span><br><span class="line">            grads[<span class="string">'b'</span> + str(idx)] = numerical_gradient(loss_w, self.params[<span class="string">'b'</span> + str(idx)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        <span class="string">"""求梯度（误差反向传播法）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x : 输入数据</span></span><br><span class="line"><span class="string">        t : 教师标签</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        具有各层的梯度的字典变量</span></span><br><span class="line"><span class="string">            grads['W1']、grads['W2']、...是各层的权重</span></span><br><span class="line"><span class="string">            grads['b1']、grads['b2']、...是各层的偏置</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        self.loss(x, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dout = <span class="number">1</span></span><br><span class="line">        dout = self.last_layer.backward(dout)</span><br><span class="line"></span><br><span class="line">        layers = list(self.layers.values())</span><br><span class="line">        layers.reverse()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设定</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">'W1'</span>], grads[<span class="string">'b1'</span>] = self.layers[<span class="string">'Conv1'</span>].dW, self.layers[<span class="string">'Conv1'</span>].db</span><br><span class="line">        grads[<span class="string">'W2'</span>], grads[<span class="string">'b2'</span>] = self.layers[<span class="string">'Affine1'</span>].dW, self.layers[<span class="string">'Affine1'</span>].db</span><br><span class="line">        grads[<span class="string">'W3'</span>], grads[<span class="string">'b3'</span>] = self.layers[<span class="string">'Affine2'</span>].dW, self.layers[<span class="string">'Affine2'</span>].db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>细心的小伙伴应该已经注意到了在上面代码的注释中提到了<strong>He初始值</strong>和<strong>xavier初始值</strong>，设置神经网络的初始值也是一门学问，设置一组优秀的初始值可以避免<strong>梯度消失</strong>的问题，目前比较常见的就是He初始值和xavier初始值，具体该什么网络选取什么初始值还需要多调研调研 。</p>
<h2 id="复杂cnn的实现"><a class="markdownIt-Anchor" href="#复杂cnn的实现"></a> 复杂CNN的实现</h2>
<p>上一节中介绍的简单的CNN网络结构在MNIST任务中已经可以取得98%的准确率，可以说十分优秀了，但是加深网络可以获得99%的准确率，当然了并不代表着网络越深，学习的效果越好，接下来实现一个负责的网络，这个网络参考了VGG网络，网络连接比较复杂，这里使用的卷积层全都是 3 × 3 的小型滤波器，特点是随着层的加深，通道数变大（卷积层的通道数从前面的层开始按顺序以 16、16、32、32、64、64 的方式增加），插入池化层，以逐渐减小中间数据的空间大小，并且，后面的全连接层中使用了 Dropout 层，并基于Adam进行最优化。</p>
<p>Dropout 是一种抑制过拟合的方法，在学习的过程中随机删除神经，训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递，每传递一次数据，就会随机选择要删除的神经元，然后在测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出，Dropout 的图示如下</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/113.png" alt></p>
<p>实现也分为forward和backward，实现代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dropout</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout_ratio=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        self.dropout_ratio = dropout_ratio</span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, train_flg=True)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> train_flg:</span><br><span class="line">            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio</span><br><span class="line">            <span class="keyword">return</span> x * self.mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x * (<span class="number">1.0</span> - self.dropout_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> dout * self.mask</span><br></pre></td></tr></table></figure>
<p>现在来说下Adam，Adam是一种参数更新的最优化方法，结合了Momentum和AdaGrad ，简单来说，Momentum以物理规律来表示梯度的移动，AdaGrad 为参数的每个元素适当地调整更新步伐，这里就直接把代码贴上去，有兴趣的小伙伴可以自行了解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""Adam (http://arxiv.org/abs/1412.6980v8)"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.001</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>)</span>:</span></span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.beta1 = beta1</span><br><span class="line">        self.beta2 = beta2</span><br><span class="line">        self.iter = <span class="number">0</span></span><br><span class="line">        self.m = <span class="literal">None</span></span><br><span class="line">        self.v = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, params, grads)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.m <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.m, self.v = &#123;&#125;, &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> key, val <span class="keyword">in</span> params.items():</span><br><span class="line">                self.m[key] = np.zeros_like(val)</span><br><span class="line">                self.v[key] = np.zeros_like(val)</span><br><span class="line">        </span><br><span class="line">        self.iter += <span class="number">1</span></span><br><span class="line">        lr_t  = self.lr * np.sqrt(<span class="number">1.0</span> - self.beta2**self.iter) / (<span class="number">1.0</span> - self.beta1**self.iter)         </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</span><br><span class="line">            <span class="comment">#self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]</span></span><br><span class="line">            <span class="comment">#self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)</span></span><br><span class="line">            self.m[key] += (<span class="number">1</span> - self.beta1) * (grads[key] - self.m[key])</span><br><span class="line">            self.v[key] += (<span class="number">1</span> - self.beta2) * (grads[key]**<span class="number">2</span> - self.v[key])</span><br><span class="line">            </span><br><span class="line">            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + <span class="number">1e-7</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias</span></span><br><span class="line">            <span class="comment">#unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias</span></span><br><span class="line">            <span class="comment">#params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)</span></span><br></pre></td></tr></table></figure>
<p>好了好了，返回之前说到的复杂网络，复杂网络可以用下图来表示</p>
<p><img src="http://www.ituring.com.cn/figures/2018/DeepLearning/144.png" alt></p>
<p>这个网络使用 He 初始值作为权重的初始值，使用 Adam 更新权重参数，把上述内容总结起来，这个网络有如下特点</p>
<ul>
<li>基于 3×3 的小型滤波器的卷积层</li>
<li>激活函数是 ReLU</li>
<li>全连接层的后面使用 Dropout 层</li>
<li>基于 Adam 的最优化</li>
<li>使用 He 初始值作为权重初始值</li>
</ul>
<p>这个复杂网络的代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepConvNet</span>:</span></span><br><span class="line">    <span class="string">"""识别率为99%以上的高精度的ConvNet</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    网络结构如下所示</span></span><br><span class="line"><span class="string">        conv - relu - conv- relu - pool -</span></span><br><span class="line"><span class="string">        conv - relu - conv- relu - pool -</span></span><br><span class="line"><span class="string">        conv - relu - conv- relu - pool -</span></span><br><span class="line"><span class="string">        affine - relu - dropout - affine - dropout - softmax</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param_1 = &#123;<span class="string">'filter_num'</span>:<span class="number">16</span>, <span class="string">'filter_size'</span>:<span class="number">3</span>, <span class="string">'pad'</span>:<span class="number">1</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param_2 = &#123;<span class="string">'filter_num'</span>:<span class="number">16</span>, <span class="string">'filter_size'</span>:<span class="number">3</span>, <span class="string">'pad'</span>:<span class="number">1</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param_3 = &#123;<span class="string">'filter_num'</span>:<span class="number">32</span>, <span class="string">'filter_size'</span>:<span class="number">3</span>, <span class="string">'pad'</span>:<span class="number">1</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param_4 = &#123;<span class="string">'filter_num'</span>:<span class="number">32</span>, <span class="string">'filter_size'</span>:<span class="number">3</span>, <span class="string">'pad'</span>:<span class="number">2</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param_5 = &#123;<span class="string">'filter_num'</span>:<span class="number">64</span>, <span class="string">'filter_size'</span>:<span class="number">3</span>, <span class="string">'pad'</span>:<span class="number">1</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 conv_param_6 = &#123;<span class="string">'filter_num'</span>:<span class="number">64</span>, <span class="string">'filter_size'</span>:<span class="number">3</span>, <span class="string">'pad'</span>:<span class="number">1</span>, <span class="string">'stride'</span>:<span class="number">1</span>&#125;,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化权重===========</span></span><br><span class="line">        <span class="comment"># 各层的神经元平均与前一层的几个神经元有连接（<span class="doctag">TODO:</span>自动计算）</span></span><br><span class="line">        pre_node_nums = np.array([<span class="number">1</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">16</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">16</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">32</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">32</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">64</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, hidden_size])</span><br><span class="line">        wight_init_scales = np.sqrt(<span class="number">2.0</span> / pre_node_nums)  <span class="comment"># 使用ReLU的情况下推荐的初始值</span></span><br><span class="line">        </span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        pre_channel_num = input_dim[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> idx, conv_param <span class="keyword">in</span> enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):</span><br><span class="line">            self.params[<span class="string">'W'</span> + str(idx+<span class="number">1</span>)] = wight_init_scales[idx] * np.random.randn(conv_param[<span class="string">'filter_num'</span>], pre_channel_num, conv_param[<span class="string">'filter_size'</span>], conv_param[<span class="string">'filter_size'</span>])</span><br><span class="line">            self.params[<span class="string">'b'</span> + str(idx+<span class="number">1</span>)] = np.zeros(conv_param[<span class="string">'filter_num'</span>])</span><br><span class="line">            pre_channel_num = conv_param[<span class="string">'filter_num'</span>]</span><br><span class="line">        self.params[<span class="string">'W7'</span>] = wight_init_scales[<span class="number">6</span>] * np.random.randn(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b7'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W8'</span>] = wight_init_scales[<span class="number">7</span>] * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b8'</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成层===========</span></span><br><span class="line">        self.layers = []</span><br><span class="line">        self.layers.append(Convolution(self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>], </span><br><span class="line">                           conv_param_1[<span class="string">'stride'</span>], conv_param_1[<span class="string">'pad'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Convolution(self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>], </span><br><span class="line">                           conv_param_2[<span class="string">'stride'</span>], conv_param_2[<span class="string">'pad'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Pooling(pool_h=<span class="number">2</span>, pool_w=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.layers.append(Convolution(self.params[<span class="string">'W3'</span>], self.params[<span class="string">'b3'</span>], </span><br><span class="line">                           conv_param_3[<span class="string">'stride'</span>], conv_param_3[<span class="string">'pad'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Convolution(self.params[<span class="string">'W4'</span>], self.params[<span class="string">'b4'</span>],</span><br><span class="line">                           conv_param_4[<span class="string">'stride'</span>], conv_param_4[<span class="string">'pad'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Pooling(pool_h=<span class="number">2</span>, pool_w=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.layers.append(Convolution(self.params[<span class="string">'W5'</span>], self.params[<span class="string">'b5'</span>],</span><br><span class="line">                           conv_param_5[<span class="string">'stride'</span>], conv_param_5[<span class="string">'pad'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Convolution(self.params[<span class="string">'W6'</span>], self.params[<span class="string">'b6'</span>],</span><br><span class="line">                           conv_param_6[<span class="string">'stride'</span>], conv_param_6[<span class="string">'pad'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Pooling(pool_h=<span class="number">2</span>, pool_w=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.layers.append(Affine(self.params[<span class="string">'W7'</span>], self.params[<span class="string">'b7'</span>]))</span><br><span class="line">        self.layers.append(Relu())</span><br><span class="line">        self.layers.append(Dropout(<span class="number">0.5</span>))</span><br><span class="line">        self.layers.append(Affine(self.params[<span class="string">'W8'</span>], self.params[<span class="string">'b8'</span>]))</span><br><span class="line">        self.layers.append(Dropout(<span class="number">0.5</span>))</span><br><span class="line">        </span><br><span class="line">        self.last_layer = SoftmaxWithLoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, train_flg=False)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="keyword">if</span> isinstance(layer, Dropout):</span><br><span class="line">                x = layer.forward(x, train_flg)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = layer.forward(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        y = self.predict(x, train_flg=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.last_layer.forward(y, t)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, x, t, batch_size=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> t.ndim != <span class="number">1</span> : t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(int(x.shape[<span class="number">0</span>] / batch_size)):</span><br><span class="line">            tx = x[i*batch_size:(i+<span class="number">1</span>)*batch_size]</span><br><span class="line">            tt = t[i*batch_size:(i+<span class="number">1</span>)*batch_size]</span><br><span class="line">            y = self.predict(tx, train_flg=<span class="literal">False</span>)</span><br><span class="line">            y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">            acc += np.sum(y == tt)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> acc / x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        self.loss(x, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dout = <span class="number">1</span></span><br><span class="line">        dout = self.last_layer.backward(dout)</span><br><span class="line"></span><br><span class="line">        tmp_layers = self.layers.copy()</span><br><span class="line">        tmp_layers.reverse()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> tmp_layers:</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设定</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, layer_idx <span class="keyword">in</span> enumerate((<span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">18</span>)):</span><br><span class="line">            grads[<span class="string">'W'</span> + str(i+<span class="number">1</span>)] = self.layers[layer_idx].dW</span><br><span class="line">            grads[<span class="string">'b'</span> + str(i+<span class="number">1</span>)] = self.layers[layer_idx].db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Hongyi Guo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://guohongyi.com/2020/11/12/用Python手撕CNN/">https://guohongyi.com/2020/11/12/用Python手撕CNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Convolutional-Neural-Network/">Convolutional Neural Network</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/11/19/用Python手撕word2vec/"><i class="fa fa-chevron-left">  </i><span>用Python手撕word2vec</span></a></div><div class="next-post pull-right"><a href="/2020/10/29/用Python手撕ANN/"><span>用Python手撕ANN</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'Rh8B8qrnJ3cukY5PM5TJziVq-gzGzoHsz',
  appKey:'zHaQG2PQjKnL9mXX2EAbKFDB',
  placeholder:'Please leave your footprints',
  avatar:'retro',
  guest_info:guest_info,
  pageSize:'5',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://guohy-1258918948.cos.ap-shanghai.myqcloud.com/59f7d76d99a79.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By Hongyi Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://guohongyi.com">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>